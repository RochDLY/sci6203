[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Les technologies mises en oeuvre sont :\nQuarto\nJupyter Notebook\nOrange Data Mining\nLes développements et les contenus sont réalisés par :\nYann Audin, doctorant à l’Université de Montréal.\nRoch Delannay, doctorant à l’Université de Montréal."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Accueil",
    "section": "",
    "text": "journal\n\n\n\n\n\n\n\nRoch Delannay\n\n\nNov 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nYann Audin\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nYann Audin\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nYann Audin\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nYann Audin\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nYann Audin & Roch Delannay\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\nRoch Delannay\n\n\nNov 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nRoch Delannay\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nRoch Delannay\n\n\nOct 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nRoch Delannay\n\n\nOct 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nRoch Delannay\n\n\nOct 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\nRoch Delannay\n\n\nOct 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nRoch Delannay\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nYann Audin et Roch Delannay\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\ninformation\n\n\n\n\n\n\n\nRoch Delannay\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\nYann Audin\n\n\nSep 21, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/get-greek-epigram-csv.html",
    "href": "posts/post-with-code/get-greek-epigram-csv.html",
    "title": "Récupération du corpus 2",
    "section": "",
    "text": "Récupération des épigrammes en grec via l’API de l’Anthologie grecque pour former notre corpus."
  },
  {
    "objectID": "posts/post-with-code/get-greek-epigram-csv.html#récupération-des-données-de-lapi",
    "href": "posts/post-with-code/get-greek-epigram-csv.html#récupération-des-données-de-lapi",
    "title": "Récupération du corpus 2",
    "section": "Récupération des données de l’API",
    "text": "Récupération des données de l’API\nLes données sont disponibles via l’URL : http://anthologiagraeca.org/api/passages.\nPour les récupérer nous les injectons sous forme de liste dans une variable results.\nL’API sert les données paginées, de fait les données récupérées sont limitées à la première page. Or nous voulons récupérer l’ensemble du corpus. Pour parer cette fonctionnalité nous créons une boucle while pour modifier l’URL de la page (vers la page suivante) et indexer toutes les données dans notre variable.\n\n# Importation of the useful libraries \n# Definition of our variables for the requests \n\nimport requests\nimport json\nimport csv\n\nurl = 'http://anthologiagraeca.org/api/passages'\nparameters = {\n    'format':'json',\n    'limit':'500'\n}\nresults = []\npagination= True\nwhile pagination == True :\n    data = requests.get(url, parameters).json()\n    for result in data['results'] :\n        results.append(result)\n    if data['next'] is None:\n        pagination = False\n    else:\n        url = data['next']"
  },
  {
    "objectID": "posts/post-with-code/get-greek-epigram-csv.html#vérification-1",
    "href": "posts/post-with-code/get-greek-epigram-csv.html#vérification-1",
    "title": "Récupération du corpus 2",
    "section": "Vérification 1",
    "text": "Vérification 1\nLa première vérification consiste à comparer le nombre de résultats obtenus avec le nombre total d’épigrammes indexées dans le portail de l’Atnhologie grecque.\n\nlen(results)\n\n4134"
  },
  {
    "objectID": "posts/post-with-code/get-greek-epigram-csv.html#écriture-dun-fichier",
    "href": "posts/post-with-code/get-greek-epigram-csv.html#écriture-dun-fichier",
    "title": "Récupération du corpus 2",
    "section": "Écriture d’un fichier",
    "text": "Écriture d’un fichier\nMaintenant que nous avons bien récupéré nos données nous souhaitons créer un fichier au format json que nous pourrons utiliser pour les futures étapes de notre fouille de texte.\nNouvel export : nous avions prévu de travailler au format JSON, mais la plupart des logiciels de fouille de textes prennent du le format CSV en entrée ! Du coup nous avons retravaillé l’export de l’API pour avoir du JSON et du CSV\n\n# Export au format json\nmesTextes = []\nfor epigram in results:\n    mesTextes.append({'url': epigram['url'], 'greekText': [text for text in epigram['texts'] if text['language'] == 'grc']})\n\n# export au format csv\nmesChamps = ['url', 'greekText']\nmesEpigrammes = []\nfor epigram in results:\n    for text in epigram['texts']:\n        if text['language'] == 'grc':\n            mesEpigrammes.append([epigram['url'], text['text']])\n\n\n# export au format json\nout_file = open('greek-epigram.json', 'w')\n\njson.dump(mesTextes, out_file, indent=3, ensure_ascii=False)\n\n# export au format csv\nwith open('EpigramsGreek.csv', 'w') as f:\n      \n    # using csv.writer method from CSV package\n    write = csv.writer(f)\n      \n    write.writerow(mesChamps)\n    write.writerows(mesEpigrammes)"
  },
  {
    "objectID": "posts/post-with-code/get-greek-epigram.html",
    "href": "posts/post-with-code/get-greek-epigram.html",
    "title": "Récupération du corpus",
    "section": "",
    "text": "Récupération des épigrammes en grec via l’API de l’Anthologie grecque pour former notre corpus."
  },
  {
    "objectID": "posts/post-with-code/get-greek-epigram.html#récupération-des-données-de-lapi",
    "href": "posts/post-with-code/get-greek-epigram.html#récupération-des-données-de-lapi",
    "title": "Récupération du corpus",
    "section": "Récupération des données de l’API",
    "text": "Récupération des données de l’API\nLes données sont disponibles via l’URL : http://anthologiagraeca.org/api/passages.\nPour les récupérer nous les injectons sous forme de liste dans une variable results.\nL’API sert les données paginées, de fait les données récupérées sont limitées à la première page. Or nous voulons récupérer l’ensemble du corpus. Pour parer cette fonctionnalité nous créons une boucle while pour modifier l’URL de la page (vers la page suivante) et indexer toutes les données dans notre variable.\n\n# Importation of the useful libraries \n# Definition of our variables for the requests \n\nimport requests\nimport json\n\nurl = 'http://anthologiagraeca.org/api/passages'\nparameters = {\n    'format':'json',\n    'limit':'500'\n}\nresults = []\npagination= True\nwhile pagination == True :\n    data = requests.get(url, parameters).json()\n    for result in data['results'] :\n        results.append(result)\n    if data['next'] is None:\n        pagination = False\n    else:\n        url = data['next']"
  },
  {
    "objectID": "posts/post-with-code/get-greek-epigram.html#vérification-1",
    "href": "posts/post-with-code/get-greek-epigram.html#vérification-1",
    "title": "Récupération du corpus",
    "section": "Vérification 1",
    "text": "Vérification 1\nLa première vérification consiste à comparer le nombre de résultats obtenus avec le nombre total d’épigrammes indexées dans le portail de l’Atnhologie grecque.\n\nlen(results)\n\n4134"
  },
  {
    "objectID": "posts/post-with-code/get-greek-epigram.html#écriture-dun-fichier",
    "href": "posts/post-with-code/get-greek-epigram.html#écriture-dun-fichier",
    "title": "Récupération du corpus",
    "section": "Écriture d’un fichier",
    "text": "Écriture d’un fichier\nMaintenant que nous avons bien récupéré nos données nous souhaitons créer un fichier au format json que nous pourrons utiliser pour les futures étapes de notre fouille de texte.\n\nmesTextes = []\nfor epigram in results:\n    mesTextes.append({'url': epigram['url'], 'greekText': [text for text in epigram['texts'] if text['language'] == 'grc']})\n\n\nout_file = open('greek-epigram.json', 'w')\n\njson.dump(mesTextes, out_file, indent=3, ensure_ascii=False)"
  },
  {
    "objectID": "posts/carnet/williams_greek-and-bio.html",
    "href": "posts/carnet/williams_greek-and-bio.html",
    "title": "Revue de littérature : Williams et al..",
    "section": "",
    "text": "Court résumé de l’article de A. C. Williams, “Identification of Ancient Greek Papyrus Fragments Using Genetic Sequence Alignment Algorithms”."
  },
  {
    "objectID": "posts/carnet/williams_greek-and-bio.html#a.-c.-williams-et-al.-2014.-identification-of-ancient-greek-papyrus-fragments-using-genetic-sequence-alignment-algorithms-2014-ieee-10th-international-conference-on-e-science",
    "href": "posts/carnet/williams_greek-and-bio.html#a.-c.-williams-et-al.-2014.-identification-of-ancient-greek-papyrus-fragments-using-genetic-sequence-alignment-algorithms-2014-ieee-10th-international-conference-on-e-science",
    "title": "Revue de littérature : Williams et al..",
    "section": "A. C. Williams et al. (2014). “Identification of Ancient Greek Papyrus Fragments Using Genetic Sequence Alignment Algorithms,” 2014 IEEE 10th International Conference on e-Science",
    "text": "A. C. Williams et al. (2014). “Identification of Ancient Greek Papyrus Fragments Using Genetic Sequence Alignment Algorithms,” 2014 IEEE 10th International Conference on e-Science\nCet article s’intéresse aux fragments de papyrus grecs endommagés, situés dans les restes du village égyptien Oxyrhynchus. Depuis une centaine d’années, seulement 10% des fragments ont pu être édités ; le travail d’identification, de transcription et d’édition des fragments abîmés est très long, il faut parfois plus d’une année pour identifier selon une méthodologie traditionnelle ne serait-ce qu’un fragment. La proposition de cette étude concerne l’identification des caractères manquants dans les fragments, en introduisant une nouvelle méthodologie basée sur les algorithmes d’alignement de séquence génétique en biologie comme méthode pour identifier le grec ancien. Le principe de cette méthode repose sur le déploiement d’une matrice de substitution qui permet de vérifier l’alignement mutuel des correspondances. Cette matrice comporte un score de probabilité d’alignement des caractères (dans le cas de la génétique il s’agit d’alignement entre les acides aminés). Si le score dépasse un certain seuil fixé par l’utilisateur, alors la paire de séquences pertinente est identifiée. L’équipe de recherche a modifié un algorithme populaire d’alignement des séquences génétiques par paires, Basic Local Alignment Search Tool (BLAST v2.2.28), dès lors baptisé Greek-BLAST. À l’origine, BLAST utilise la famille de matrices BLOSUM comme matrice de substitution, laquelle sera utilisée comme base pour créer la matrice de substitution GLOSUM (Greek Letter Oriented Substitution Matrix) afin de calculer la fréquence cible pour chaque paire de lettres (à partir des travaux réalisés à l’Université d’Oxford lors du projet Ancient Lives). Après entrainement et application de la méthodologie sur un corpus simul de 14 100 fragments, les résultats obtenus sont divisés en 5 catégories de fragments : non-édité, édité selon un taux d’erreur, taux de caractères supplémentaires, taux d’écart vertical. Les taux d’identification dépassent les 86%, le plus faible étant de 86,9% pour les fragments catégorisés selon le taux d’écart vertical et le plus élevé, sans surprise, pour les fragments non édités est de 89,6%. En conclusion de cet articles, les auteur.e.s sont satisfait de la méthodologie mise en place, tout en notant que d’autres algorithmes d’alignement de séquences génétiques profiteraient de cette approche."
  },
  {
    "objectID": "posts/carnet/test-orange.html",
    "href": "posts/carnet/test-orange.html",
    "title": "Test du logiciel Orange",
    "section": "",
    "text": "Premiers tests avec l’outil Orange Data Mining."
  },
  {
    "objectID": "posts/carnet/test-orange.html#installation",
    "href": "posts/carnet/test-orange.html#installation",
    "title": "Test du logiciel Orange",
    "section": "Installation",
    "text": "Installation\nL’installation du logiciel sur ma machine s’est déroulée sans accroc : la manipulation est très simple, il suffit de lancer la commande pip install orange3 dans le terminal, puis une fois les paquets installés, il faut utiliser la commande python -m Orange.canvas et le tour est joué ! Le logiciel se lance !"
  },
  {
    "objectID": "posts/carnet/test-orange.html#principe-de-fonctionnement",
    "href": "posts/carnet/test-orange.html#principe-de-fonctionnement",
    "title": "Test du logiciel Orange",
    "section": "Principe de fonctionnement",
    "text": "Principe de fonctionnement\nOrange est basé sur le principe de visual programming (un peu comme les logiciels Pure Data ou Max/MSP). Une fenêtre s’ouvre et l’utilisateur doit assembler des widgets dans le plan de travail pour créer son workflow.\nTout workflow est initié par l’importation des données, auxquelles seront appliqués différents traitements en vue d’obtenir les résultats escomptés. Orange est tout autant utilisé pour faire de la fouille de données que de la fouille de textes.\nCi-dessous un premier test réalisé avec un des jeux de données (test) fourni par le logiciel.\n\nLe jeu de données est un corpus de texte au format .tab.\nLe premier widget que nous appliquerons au corpus sera le preprocess text.\n\nCette phase permet de transformer notre texte (application du lowercase pour enlever les majuscules), de le tokeniser grâce à l’expression régulière \\w+ (pour matcher avec tous les caractères), puis de le filtrer grâce à une liste de stopwords en anglais.\nLe deuxième widget sera appliqué au corpus preprocessé, il s’agit du widget bag of words\n\nBag of words permet de générer un compteur pour chaque donnée du document.\nLe troisième widget de ce workflow s’appelle Distances.\n\nIl permet de calculer la distance entre les document (cosinus). Plus la distance est petite, plus les documents sont proches et similaire.\nLe quatrième widget se nomme Hierarchical Clustering.\n\nCette étape marque le début de la visualisation des résultats après transformation des données : elle permet de visualiser la hiérarchie des documents à travers un dendrogramme.\nLe dernier widget, encore une visualisation, est intitulé MDS.\n\nIl offre une visualisation spatiale de nos données."
  },
  {
    "objectID": "posts/carnet/test-orange.html#commentaires",
    "href": "posts/carnet/test-orange.html#commentaires",
    "title": "Test du logiciel Orange",
    "section": "Commentaires",
    "text": "Commentaires\nUn autre test a été réalisé avec notre corpus d’épigrammes au format .csv (français, anglais, grec ancien) et il n’a pas été aussi simple d’appliquer ce workflow à nos données. Les résultats obtenus ne faisaient pas sens : autant la génération d’un nuage de mot (avec le widget wordcloud) est très facile, autant les calculs de distance et la création de clusters hiérarchisés ne fonctionnait pas. Le problème est relativement simple à cibler : le corpus n’est pas annoté par des étiquettes, alors que le corpus fourni par Orange l’est.\nNotre prochaine étape de travail sera donc de traiter notre corpus pour le rendre “intelligible” pour ce logiciel.\nPremier ressenti face au logiciel : Orange est un logiciel confortable à utiliser, la prise en main se fait très rapidement et la documentation (très fournie) accompagne l’utilisateur tout au long de son apprentissage. On trouve des informations sur le site web du logiciel ainsi qu’une multitude de petits tutoriels sur YouTube pour expliquer chacune des étapes. Cependant, dès les premières manipulations, j’ai ressenti une légère frustration (relative au visual programming). L’agencement des widgets entre eux permet de bien voir le parcours de nos données, mais cela rend les calculs appliqués opaques. N’étant pas un expert dans le domaine de la fouille de texte (je dirais plutôt un néophyte) il est plus difficile de comprendre comment fonctionne exactement chaque widget. Par exemple, le widget Distances ne propose qu’un petit sélecteur pour le type de distance à calcul, mais que se passe-t-il sous ce sélecteur ? J’obtiens des résultats, mais sans comprendre comment le logiciel les a générés… affaire à suivre !"
  },
  {
    "objectID": "posts/carnet/fayyad.html",
    "href": "posts/carnet/fayyad.html",
    "title": "From Data Mining to Knowledge Discovery in Database: Une approche littéraire",
    "section": "",
    "text": "Résumé de l’article de Fayyad."
  },
  {
    "objectID": "posts/carnet/fayyad.html#from-data-mining-to-knowledge-discovery-in-databases",
    "href": "posts/carnet/fayyad.html#from-data-mining-to-knowledge-discovery-in-databases",
    "title": "From Data Mining to Knowledge Discovery in Database: Une approche littéraire",
    "section": "From Data Mining to Knowledge Discovery in Databases",
    "text": "From Data Mining to Knowledge Discovery in Databases\nCe n’est pas la première fois que je rencontre ce texte classique de Fayyad, Piatetsky-Shapiro et Smyth qui semble être considéré comme l’entrée en matière par excellence depuis sa publication en 1996 avec plus de 12 000 citations répertoriées sur Scholar en 2022. Nous y retrouvons un survol du champ tel qu’il existait en 1996, une retrospective qui reste actuelle, les bases de l’exploration de données restant somme toute similaire à ce qu’elles étaient à la fin du dernier siècle.près avoir couvert plusieurs exemples d’utilisations des méthodes de minage de données et les types de résultats qui peuvent en être dérivés, l’article décortique les étapes du processus de découverte de connaissance. De l’acquisition de données à l’interprétation des savoirs rendus visibles par les algorithmes, l’accent est mis sur les choix humains et leurs conséquences. Un manque de discernement de la part du chercheur peut mener à des connaissances inutiles, inexplicables ou simplement fausses ; celui-ci doit donc comprendre les modèles qu’il utilise ainsi que les données qu’il traite avec ces derniers. Finalement, les auteurs survolent plusieurs approches différentes à la découverte de connaissance dans les bases de données à l’aide d’un exemple simple. Ainsi, l’article démontre que différentes méthodes donnent différents résultats (et même différentes formes de connaissances) pour les mêmes données."
  },
  {
    "objectID": "posts/carnet/fayyad.html#lecture-critique-de-larticle",
    "href": "posts/carnet/fayyad.html#lecture-critique-de-larticle",
    "title": "From Data Mining to Knowledge Discovery in Database: Une approche littéraire",
    "section": "Lecture critique de l’article",
    "text": "Lecture critique de l’article\nLa découverte de connaissances dans les bases de données offre deux promesses des plus attirantes aux chercheurs des sciences humaines. D’un côté, la possibilité de confirmer ou d’infirmer des modèles théoriques à l’aide d’une étude statistique de leurs objets. De l’autre, la découverte de connaissances qui ne sont pas supportées ou prise en compte par la théorie, et donc l’ouverture de nouveaux horizons théoriques. Ce processus dans le contexte des sciences humaines est décrit par les auteurs comme un processus lent et précis : la prudence est de mise pour générer des connaissances réelles et utiles. Un chercheur se doit de faire des choix cohérents à chaque étape du processus en gardant toujours au centre de ses décisions deux valeurs : la vérité et l’éthique. Ainsi, les humanités numériques sont perpétuellement en questionnement, elles doutent d’elles-mêmes par défaut tant pour assurer la qualité de leurs résultats que pour protéger les sujets de leurs recherches. Cela est limitant dans plusieurs domaines qui pourraient profiter de plus de libertés expérimentales tout en protégeant les potentiels sujets qui pourraient en souffrir. Ces questions éthiques et épistémologiques ne sont pas partagées par d’autres domaines qui font usage de la découverte de connaissance dans les bases de données. Les valeurs au cœur d’une démarche la transforme radicalement, il y a donc une différence radicale entre le développement d’outils numériques de recherche et ceux développés par les secteurs privés. Dans ce dernier cas, les motivations premières qui guide l’écriture des algorithmes servent la métrique du profit et ont l’avantage de pouvoir être raffiné rapidement alors que différentes méthodes peuvent être testées à grandes échelles sans égard pour l’éthique. De bien des façons, les expériences algorithmiques de géants du web font de leurs utilisateurs des rats de laboratoire à l’échelle mondiale sacrifié à l’autel de l’argent."
  },
  {
    "objectID": "posts/carnet/28septembre2022.html",
    "href": "posts/carnet/28septembre2022.html",
    "title": "Partie 1. Définition de la problématique et présentation de travaux reliés",
    "section": "",
    "text": "Consignes et plan pour le premier rendu."
  },
  {
    "objectID": "posts/carnet/28septembre2022.html#évaluation-pour-le-2022-10-13",
    "href": "posts/carnet/28septembre2022.html#évaluation-pour-le-2022-10-13",
    "title": "Partie 1. Définition de la problématique et présentation de travaux reliés",
    "section": "Évaluation pour le [2022-10-13]",
    "text": "Évaluation pour le [2022-10-13]\nDéfinir la problématique et présenter 10 travaux (plus ou moins deux) en lien avec la problématique.\n\nConsignes\n\nRédiger entre 4 ou 5 pages pour l’état de l’art :\n\nPour chaque texte de la bibliographie commentée :\n\nCorpus\nMéthodes\nRésultats\n\n\nRédiger entre 1 ou 2 pages pour détailler la problématique."
  },
  {
    "objectID": "posts/carnet/Rodda_2017.html",
    "href": "posts/carnet/Rodda_2017.html",
    "title": "Revue de littérature : Rodda et al.",
    "section": "",
    "text": "Court résumé de l’article de Rodda, Martina A., Marco S. G. Senaldi et Alessandro Lenci (2017)."
  },
  {
    "objectID": "posts/carnet/Rodda_2017.html#panta-rei-tracking-semantic-change-with-distributional-semantics-in-ancient-greek",
    "href": "posts/carnet/Rodda_2017.html#panta-rei-tracking-semantic-change-with-distributional-semantics-in-ancient-greek",
    "title": "Revue de littérature : Rodda et al.",
    "section": "Panta rei: Tracking Semantic Change with Distributional Semantics in Ancient Greek",
    "text": "Panta rei: Tracking Semantic Change with Distributional Semantics in Ancient Greek\nDans le cadre du présent projet, cet article a plusieurs fonctions :\n\nJustifier l’utilisation de la vectorisation de mots dans le contexte d’un corpus limité en grec ancien et en exemplifier les principes;\nJustifier la vectorisation de mots tirée de la linguistique distributionnelle comme une représentation adjacente à la sémantique;\nConfirmer que la linguistique distributionnelle permet la caractérisation des textes et mots d’un corpus;\nDénoter des limites de la vectorisation de mot dans les petits corpus.\n\nLes auteurs utilisent les textes classiques grecs trouvés dans le TLG (Thesaurus Linguae Graecae), un projet de l’Université de Californie cherchant à compiler un corpus exhaustif de la littérature en grec ancien.\nL’article est une étude cherchant à déterminer comment le lexique grec a changé avec l’avènement du christianisme et se concentrait sur des oeuvres et des traités : “The pre-Christian sub-corpus contains 6,795,253 tokens, while the Christian sub-corpus totalizes 29,051,269 tokens” [@martina_a_rodda_panta_2017]. Leur méthodologie repose sur la modélisation des mots dans un espace vectoriel en deux temps, d’abord en lemmatisant ces derniers à l’aide de Morpheus, puis en créant deux modèles vectoriels à 300 dimensions avec le DISSET toolkit (un pré-chrétienté, et un post-chrétienté). Ce modèle fut ensuite analysé à partir de la méthode RSA (Representational Similarity Analysis) qui permet de découvrir les termes ayant le plus changé entre les deux sous-corpus. Certains des termes ayant souffert d’une dérive linguistique forte confirment les intuitions des chercheurs qui les séparent en deux catégories vagues : les termes chrétiens et les termes techniques. Par exemple, “παραβολή (parabolé)” change de sens entre les deux ères considérées, signifiant d’abord “comparaison”, puis “parabole”. Similairement, le terme “ὑπóστασις (hypostasis)”, qui signifiant “fondation” avant l’ère chrétienne, signifiera “substance” durant cette dernière.\nPlusieurs des différences trouvées, cette fois en analysant chaque terme en fonction de ses voisins les plus proches dans les deux espaces linguistiques, témoignent d’une profonde dérive linguistique du grec ancien. Encore, deux catégories attirent l’attention des chercheurs : les termes chrétiens et techniques (les auteurs citent en exemple les dérives de “πνεῦμα” qui signifiait “souffle” et devient “esprit, et de”δύναμις” qui signifiait “pouvoir” et devient “propriété”.\nFinalement, à l’aide d’un graphique de type t-SNE1, les auteurs sont en mesure de trouver des grappes de termes parmi ceux ayant le plus changé. Leur étude fait état d’une dérive linguistique liée à l’avènement de la chrétienté, mais aussi de transformations importantes dans les domaines de la médecine, de l’astronomie, de la géométrie et de la philosophie. Dans leur conclusion, les auteurs indiquent : “From a methodological standpoint, the fact that the results obtained from such a small corpus of purely literary texts are both meaningful and informative is of great relevance.”"
  },
  {
    "objectID": "posts/carnet/tp02-part2.html",
    "href": "posts/carnet/tp02-part2.html",
    "title": "Corpus de l’Anthologie grecque : exploration des données",
    "section": "",
    "text": "Compte-rendu du TP02 : description du corpus de données."
  },
  {
    "objectID": "posts/carnet/tp02-part2.html#présentation-du-corpus",
    "href": "posts/carnet/tp02-part2.html#présentation-du-corpus",
    "title": "Corpus de l’Anthologie grecque : exploration des données",
    "section": "Présentation du corpus",
    "text": "Présentation du corpus\n\nRéférences\n\nVitali-Rosati, Marcello, Elsa Bouchard, Christian Raschle. “Pour une édition numérique collaborative de l’Anthologie grecque.” Chaire de recherche du Canada sur les écritures numériques, accédé le 9 novembre 2022, <url = https://anthologiagraeca.org/>.\n\nPour une liste complète des collaborateurs (coordination, développement, éditeurs, partenaires, etc.), consulter le site web https://anthologiagraeca.org/pages/equipe-et-partenaires/.\nUne grande partie des textes en grec ancien et des traductions en anglais sont tirés de :\n\nPaton, W. R. The Greek Anthology. Harvard University Press; W. Heinemann, New York, 1916-18. (Cinq volumes)\n\n\n\nLe manuscrit\nL’Anthologie grecque est un recueil qui regroupe la poésie épigrammatique grecque issue de la période classique jusqu’à la période byzantine, soit près de 4000 pièces, de 325 auteurs différents, s’étalant sur plus de 15 siècles. Telle que nous la possédons aujourd’hui, l’Anthologie a une histoire complexe (Cameron 1993). L’expression « Anthologie grecque » désigne l’ensemble constitué par deux parties. D’une part, l’Anthologie palatine, un manuscrit datant du Xe siècle (le Codex Palatinus 23) retrouvé en 1606 par Claude Saumaise à la Bibliothèque palatine d’Heidelberg. D’autre part, l’Appendix Planudea, soit les épigrammes absentes du manuscrit palatin, présentes dans l’Anthologie de Planude, une compilation datant du début du XIVe siècle, rédigée par Maxime Planude (Aubreton 1968; Beta 2019).\nMonument de la littérature, l’Anthologie est le document principal qui nous transmet la poésie épigrammatique antique (K. Gutzwiller 1997; K. J. Gutzwiller 1998). Né en Grèce, le genre de l’épigramme évolue de manière significative dans l’histoire de la littérature grecque, mais également au sein même de l’Anthologie. Initialement composée pour être gravée (epigramma signifie « texte inscrit sur un objet »), l’épigramme se présente comme un petit poème, simple, composé d’une ou de deux lignes. Les premières épigrammes étaient des épitaphes ou des inscriptions accompagnant une statue, un don ou un ex-voto. Vers le VIe siècle, ces inscriptions se versifient. Dès l’époque hellénistique, l’épigramme se détache de son support, se diversifie dans son propos et devient un véritable genre littéraire, caractérisée par sa brièveté (brevitas) et son trait piquant (argutia), et se grave désormais non pas dans le marbre, mais dans l’esprit du lectorat.\nL’Anthologie grecque offre plusieurs thèmes, sujets et registres de langues ; par exemple, le premier livre rassemble les épigrammes chrétiennes et le cinquième des épigrammes amoureuses ou érotiques. Les derniers livres sont moins nettement divisés par thèmes, une conséquence de leur addition tardive au projet. Ainsi, le langage n’est pas consistant, allant de l’oraison funéraire à la blague mathématique en passant par les descriptions littéraires et la théologie chrétienne.\nEn plus de présenter un intérêt certain pour l’étude du genre épigrammatique (Prioux 2008), l’Anthologie grecque constitue un corpus précieux et diversifié de formes intertextuelles, dont celle de la variation. Plusieurs des auteurs de l’Anthologie s’inspirent en effet les uns des autres (Waltz 1960). La variatio, forme très spécifique d’intertextualité commune et prisée dans la littérature grecque, consiste à reprendre une pièce d’un autre auteur et de la réécrire avec des variations stylistiques, rhétoriques ou paradigmatiques (Laurens 1989). Le procédé était particulièrement apprécié des épigrammatistes : la simplicité de la forme permettait aux auteurs de s’illustrer en l’espace de quelques vers. Ainsi, l’épigramme est souvent décrite comme un art de la variation en tant que tel (Tarán 1979). La nature même du corpus anthologique (la réunion de poèmes hétéroclites néanmoins reliés par des topoï communs) en fait une source d’intertextualité inépuisable. Certains thèmes reviennent particulièrement fréquemment, et les épigrammes se répondent, parfois avec plusieurs siècles de décalage (K. J. Gutzwiller 1998). Cette forme (qui s’apparente presque à du plagiat), bien vue et encouragée par les pratiques rhétoriques, commande la production de la littérature grecque et son évolution (Laurens 1989).\n\n\nDescription de l’API\nLa Chaire de recherche du Canada sur les écritures numériques porte depuis 2014 un projet de recherche initialement intitulé “Édition numérique collaborative de l’Anthologie Palatine et de ses multiples vérités”, puis “Pour une édition numérique collaborative de l’Anthologie grecque” suite à l’intégration du manuscrit de Planude dans le projet. Il existe plusieurs différences entre le manuscrit décrit dans la section précédente et l’API utilisée dans le présent projet de fouille de texte. “Pour une édition numérique collaborative de l’Anthologie grecque” offre une collection de 4134 épigrammes en grec ancien rédigées par 310 auteurs plutôt que 325. Ce sont en fait 309 auteurs qui sont nommés dans l’API, le 310ème regroupant les auteurs anonymes. Le projet rend disponible le manuscrit et l’Anthologie sous plusieurs formes, des images des manuscrits originaux, les textes en langue originale, des traductions et des informations telles que l’auteur, des listes de mots clés, des commentaires philologiques, etc. Ces données sont disponibles sur le site web du projet et sous forme d’API (Application Programming Interface) à https://anthologiagraeca.org/api/. Chacune des épigrammes devient un fragment accessible au moyen d’une requête GET. Ce dispositif est documenté sur un site web dédié.\nL’ensemble de ce manuscrit est disponible en accès libre. Nous remercions les partenaires du projet :\n\nLa Chaire de recherche du Canada sur les écritures numériques, porteuse du projet de recherche et de l’API ;\nLa Bibliothèque numérique Perseus, grâce à laquelle les textes en grec et en anglais ont pu être récupérés ;\nLa Bibliothèque Palatine (Heidelberg), qui rend disponible les épigrammes sous forme de fragment (image) au format IIIF.\n\n\n\nRécupération des données\nLes données sont disponibles via l’API à l’URL suivante : http://anthologiagraeca.org/api/passages. Pour ne pas travailler uniquement depuis l’API1, nous avons décidé d’exporter l’ensemble des épigrammes du corpus dans un fichier que nous garderons en local. Pour ce faire, nous avons réalisé un script en python décrit ci-dessous. L’ensemble des données de l’API sont accessibles via l’endpoint, pour les récupérer, nous les injectons sous forme de liste dans une variable results. L’API sert les données paginées, de fait les données récupérées sont limitées à la première page, or, nous voulons récupérer l’ensemble du corpus. Pour parer cette fonctionnalité, nous créons une boucle while pour modifier l’URL de la page (vers la page suivante) et indexer toutes les données de chacune des pages de l’API dans notre variable.\n# Importation des librairies python nécessaires\n# Définition des variables pour les requêtes\n\nimport requests\nimport json\nimport csv\n\nurl = 'http://anthologiagraeca.org/api/passages'\nparameters = {\n    'format':'json',\n    'limit':'500'\n}\nresults = []\npagination= True\nwhile pagination == True :\n    data = requests.get(url, parameters).json()\n    for result in data['results'] :\n        results.append(result)\n    if data['next'] is None:\n        pagination = False\n    else:\n        url = data['next']\nLa première vérification consiste à comparer le nombre de résultats obtenus avec le nombre total d’épigrammes indexées dans le portail de l’Anthologie grecque.\nlen(results)\n    4134\nMaintenant que nous avons bien récupéré nos données, nous souhaitons créer un fichier au format JSON que nous pourrons utiliser pour les futures étapes de notre fouille de texte.\nNouvel export : nous avions prévu de travailler uniquement avec le format JSON, mais la plupart des logiciels de fouille de textes prennent en charge le format CSV en entrée ! Ainsi, nous avons retravaillé l’export de l’API pour avoir du JSON et du CSV.\n# Export au format json\nmesTextes = []\nfor epigram in results:\n    mesTextes.append({'url': epigram['url'], 'greekText': [text for text in epigram['texts'] if text['language'] == 'grc']})\n\n# Export au format csv\nmesChamps = ['url', 'greekText']\nmesEpigrammes = []\nfor epigram in results:\n    for text in epigram['texts']:\n        if text['language'] == 'grc':\n            mesEpigrammes.append([epigram['url'], text['text']])\n# Export au format json\nout_file = open('greek-epigram.json', 'w')\n\njson.dump(mesTextes, out_file, indent=3, ensure_ascii=False)\n\n# Export au format csv\nwith open('EpigramsGreek.csv', 'w') as f:\n      \n    # Utilisation de la méthode csv.writer de la librairie CSV\n    write = csv.writer(f)\n      \n    write.writerow(mesChamps)\n    write.writerows(mesEpigrammes)\nLe résultat obtenu avec ce script comporte un document JSON et un document CSV contenant les épigrammes grecques et les URL (via l’API) qui permettent de les identifier.\n\n\nInformations précises sur le corpus\nL’Anthologie grecque telle que présente dans l’API comporte 4134 épigrammes réparties entre 16 livres, présentés dans le présent graphique.\n\nCette visualisation ne représente pas la longueur de ces épigrammes, le livre 2 par exemple est divisé entre seulement 10 épigrammes, ces derniers sont toutefois longs de 257.5 mots2.\n\n\n\nLivre\nÉpigrammes\nLongueur moyenne en mots\n\n\n\n\n1\n123\n55.41\n\n\n2\n10\n257.50\n\n\n3\n19\n37.53\n\n\n4\n5\n166.40\n\n\n5\n310\n64.67\n\n\n6\n359\n49.36\n\n\n7\n758\n59.11\n\n\n8\n258\n31.67\n\n\n9\n831\n46.79\n\n\n10\n127\n38.11\n\n\n11\n443\n44.29\n\n\n12\n260\n45.95\n\n\n13\n32\n41.47\n\n\n14\n151\n45.03\n\n\n15\n52\n93.87\n\n\n16\n396\n41.13\n\n\n\nÀ titre de comparaison, la longueur moyenne des épigrammes est 30.69 mots. L’Anthologie grecque suit une distribution intéressante quant à la longueur des épigrammes, trois groupes principaux étant visibles : soit les épigrammes de moins de 17 mots, ceux de plus de 30, et finalement ceux entre ces deux groupes.\n\nL’API est un projet incomplet auquel des modifications sont encore apportées presque toutes les semaines, en ce sens, il est important de noter que cinq des épigrammes n’ont pas de textes en grec qui leur sont associés dans le JSON. Les épigrammes 7.0 et 11.0 n’ont simplement pas de texte en grec qui leur sont associés, bien que des images de fragments existent pour chacun. Similairement, le texte 16.10 contient uniquement les caractères “῀.”. Finalement, deux épigrammes du livre 11 ont vu leurs textes en grec être encodé “eng” plutôt que “grc” de sorte qu’une recherche automatique ne découvre pas de texte en grec ancien pour les fragments 376 et 382 de ce livre. Dans le cadre de notre fouille de texte en python, ces erreurs étaient assez peu nombreuses pour que nous puissions les corriger à la main.\n\n\nPrétraitement avec Orange Data Mining\nLe corpus au format CSV est brut, nous y trouvons seulement les épigrammes grecques et leur URL pour les identifier, mais nous aurons certainement besoin de le compléter avec d’autres informations et/ou de le réduire pour correspondre au mieux à notre problématique sur les variations.\n\n\nAfin de comprendre comment nous allons pouvoir travailler avec cet objet, nous avons décidé d’effectuer des premiers tests de preprocessing avec le logiciel Orange Data Mining. Orange est un outil de programmation visuelle, l’utilisation de widgets dans l’espace de travail du logiciel permet de créer un workflow et d’appliquer une suite d’opérations aux données du corpus. Nous avons suivi l’un des workflows proposé en exemple dans la documentation du logiciel afin d’observer la forme des résultats que nous pouvons obtenir. Les widgets utilisés sont les suivants :\n\nPreprocess Text : ce widget permet de transformer (lowercase), de tokeniser (expression régulière \\w+), de normaliser (lemmatiser selon les lemmes grecs) et de filtrer (liste de stopwords créée par l’équipe de la CRCEN, et suppression des tokens en dehors d’une certaine fréquence) le corpus d’épigrammes. Avec 4218 instances en entrée, nous obtenons 64637 tokens et 36279 types. Ce résultat n’a jamais été deux fois le même selon les paramètres utilisés.\n\n\n\nOrange permet d’ores et déjà une première visualisation du corpus sous forme de nuage de mots grâce au widget Word Cloud.\n\n\n\nLe widget suivant s’appelle Bag of Words, il crée un compteur de la fréquence de chaque terme du corpus.\nDistances calcule la distance (cosinus dans notre cas) entre chaque ligne du fichier source : il détermine la proximité entre les lignes.\nHierarchical Clustering offre une visualisation sous forme de cluster des distances calculées précédemment.\n\n\n\nL’avant dernier widget, Corpus Viewer permet une visualisation plus fine des clusters : en sélectionnant un cluster particulier ou un ensemble de clusters, il est possible de les afficher sous forme de données brutes dans une nouvelle fenêtre.\n\n\n\nLe dernier widget MDS, mal placé dans cet exemple, est une autre technique de regroupement. Nous développerons ultérieurement cette approche.\n\n\nLes résultats que nous obtenons permettent de faire un premier état des lieux sur notre corpus : il nous faut correctement composer ce dernier pour commencer à réellement travailler à la problématique sur les variations. Toutefois, les regroupements par clusters (à partir des distances par cosinus) ne sont pas sans intérêt, en effet, nous remarquons que les clusters regroupent des mot-clefs appartenant à un même champ lexical. Si nous reprenons l’exemple affiché précédemment, les épigrammes du cluster sont articulées autour des mots clefs “Muse”, “peinture”, “écriture”. Cette méthode d’apprentissage automatique fait partie de l’ensemble des méthodes non supervisées (conseillée à la fin du TP01) et fera l’objet d’une attention particulière pour les prochains développements de cette étude."
  },
  {
    "objectID": "posts/carnet/Johnson_2021.html",
    "href": "posts/carnet/Johnson_2021.html",
    "title": "Revue de littérature : Johnson et al.",
    "section": "",
    "text": "Court résumé de l’article de Johnson, Kyle et al. (2021)."
  },
  {
    "objectID": "posts/carnet/Johnson_2021.html#the-classical-language-toolkit-an-nlp-framework-for-pre-modern",
    "href": "posts/carnet/Johnson_2021.html#the-classical-language-toolkit-an-nlp-framework-for-pre-modern",
    "title": "Revue de littérature : Johnson et al.",
    "section": "The Classical Language Toolkit: An NLP Framework for Pre-Modern",
    "text": "The Classical Language Toolkit: An NLP Framework for Pre-Modern\nLe Classical Language ToolKit (CLTK) est une librairie Python qui émule les fonctions de traitement textuel automatique du Natural Language TookKit, mais mets l’accent sur les langues pré-modernes (anciennes, mortes ou historiques). Contrairement aux langues vivantes encore changeantes, les langues couvertes par CLTK ne changent plus, ce qui permet une certaine stabilité des modèles. Toutefois, bien que de nouveaux textes ne s’ajoutent plus au corpus, ces derniers ont subi de fortes variations, autant au fil des années que selon les auteurs et régions.\nLa librairie CLTK a été produite en tenant compte de la taille des corpus dans les langues qu’elle couvre ; elle met à disposition des chercheurs plusieurs outils algorithmiques, certains provenant d’autres librairies de Natural Language Processing. D’un point de vue technique, l’architecture de CLTK se sépare en cinq types de données créées spécifiquement pour cette librairie, ainsi qu’une classe (NLP). C’est à partir de cette interface qu’un utilisateur peut appeler différentes fonctions (appelés des processus) pouvant accomplir des tâches utiles aux philologues. 19 différents canaux de processus (pour les 19 langues préconfigurées dans CLTK) permettent d’extraire des caractéristiques textuelles ou d’enrichir le modèle, par exemple en lemmatisant ce dernier. Les processus de CLTK transforment les objets DOC produit par la fonction “analyse” en y ajoutant des données supplémentaires, mais ce ne sont pas là les seules fonctions de la classe NPL. “FetchCorpus” est particulièrement intéressant puisqu’elle permet de récupérer les modèles générés par d’autres utilisateurs, ce qui peut considérablement accélérer la recherche. En effet, plusieurs formes de modèles linguistiques demandent des corpus massif ainsi qu’une importante puissance de calcul ; en utilisant les travaux d’autres chercheurs, un travail considérable peut être évité."
  },
  {
    "objectID": "posts/carnet/laurens.html",
    "href": "posts/carnet/laurens.html",
    "title": "Revue de littérature : Laurens",
    "section": "",
    "text": "Court résumé du chapitre 3 de L’Abeille dans l’ambre."
  },
  {
    "objectID": "posts/carnet/laurens.html#laurens-p.-2012.-le-modèle-et-la-variation-dans-labeille-dans-lambre.-les-belles-lettres.",
    "href": "posts/carnet/laurens.html#laurens-p.-2012.-le-modèle-et-la-variation-dans-labeille-dans-lambre.-les-belles-lettres.",
    "title": "Revue de littérature : Laurens",
    "section": "Laurens, P. (2012). “Le modèle et la variation”, Dans L’Abeille dans l’ambre. Les Belles Lettres.",
    "text": "Laurens, P. (2012). “Le modèle et la variation”, Dans L’Abeille dans l’ambre. Les Belles Lettres.\n“Le modèle et la variation” est le troisième chapitre de l’ouvrage L’Abeille dans l’ambre, rédigé par Pierre Laurens. Cet ouvrage poursuit une réflexion menée sur “l’essence” du style épigrammatique, notamment celles de l’Anthologie grecque, qui s’est étalé sur presque deux millénaires entre l’époque alexandrine et la fin de la Renaissance. Plus que retracer son histoire, Pierre Laurens analyse et découpe l’évolution de ce style littéraire. Dans le chapitre qui nous intéresse, Laurens propose de modéliser une typologie des entre modèle et variations des épigrammes. Comme mentionné en introduction, la variation est un art rhétorique prisé des poètes grecs, génère tour à tour concurrence ou dialogue entre eux et cela de manière diachronique. La typologie décrite par l’auteur comporte trois niveaux distincts de variation, le premier niveaux étant le styliste, le deuxième rhétorique et le dernier res ou paradigmatique.\n\nLa variation stylistique porte sur les mots et leur agencement : “déplacements d’éléments invariables, substitutions d’ordre lexical ou stylistique” (p. 124) ;\nLa variation rhétorique s’attache à démultiplier à l’infini les possibilités d’expressions d’une même idée (p.127).\nLa variation paradigmatique concerne la dernière variable de l’épigramme : le sujet. Il s’agit ici de changer le sujet de l’épigramme, appelant ainsi un nouveau champ lexical adapté à ce nouveau sujet. Les variations de ce niveau ont un potentiel infini (p. 128).\n\nCette typologie devient un outil de catégorisation des variations qui, à partir d’un modèle, sera utilisée dans notre projet de fouille de texte pour identifier lesdites variations dans le corpus anthologique."
  },
  {
    "objectID": "posts/carnet/presentation.html",
    "href": "posts/carnet/presentation.html",
    "title": "Présentation du sujet",
    "section": "",
    "text": "Présentation et probématisation du projet de fouille de texte"
  },
  {
    "objectID": "posts/carnet/presentation.html#présentation-du-sujet",
    "href": "posts/carnet/presentation.html#présentation-du-sujet",
    "title": "Présentation du sujet",
    "section": "Présentation du sujet",
    "text": "Présentation du sujet\n\nProjet - Corpus\nL’Anthologie grecque est un recueil qui regroupe la poésie épigrammatique grecque issue de la période classique jusqu’à la période byzantine, soit près de 4000 pièces, de 325 auteurs différents, s’étalant sur plus de 15 siècles. Telle que nous la possédons aujourd’hui, l’Anthologie a une histoire complexe (Cameron, 1993). L’expression « Anthologie grecque » désigne l’ensemble constitué par deux parties. D’une part, l’Anthologie palatine, un manuscrit datant du Xe siècle (le Codex Palatinus 23) retrouvé en 1606 par Claude Saumaise à la Bibliothèque palatine de Heidelberg. D’autre part, l’Appendix Planudea, soit les épigrammes absentes du manuscrit palatin présentes dans l’Anthologie de Planude, une compilation datant du début du XIVe siècle, rédigée par Maxime Planude (Aubreton, 1968 ; Beta, 2019).\nMonument de la littérature, l’Anthologie est le document principal qui nous transmet la poésie épigrammatique antique (Gutzwiller, 1997 ; Gutzwiller, 1998). Né en Grèce, le genre de l’épigramme évolue de manière significative dans l’histoire de la littérature grecque, mais également au sein même de l’Anthologie. Initialement composée pour être gravée (epigramma signifie « texte inscrit sur un objet »), l’épigramme se présente comme un petit poème, simple, composé d’une ou de deux lignes. Les premières épigrammes étaient des épitaphes ou des inscriptions accompagnant une statue, un don ou un ex-voto. Vers le VIe siècle, ces inscriptions se versifient. Dès l’époque hellénistique, l’épigramme se détache de son support, se diversifie dans son propos et devient un véritable genre littéraire, caractérisée par sa brièveté (brevitas) et son trait piquant (argutia), et se grave désormais non pas dans le marbre, mais dans l’esprit du lectorat.\nEn plus de présenter un intérêt certain pour l’étude du genre épigrammatique (Prioux, 2008), l’Anthologie grecque constitue un corpus précieux et diversifié de formes intertextuelles, dont celle de la variation. Plusieurs des auteurs de l’Anthologie s’inspirent en effet les uns des autres (Waltz, 1960). La variatio, forme très spécifique d’intertextualité commune et prisée dans la littérature grecque, consiste à reprendre une pièce d’un autre auteur et de la réécrire avec des variations stylistiques, rhétoriques ou paradigmatiques (Laurens, 2012). Le procédé était particulièrement apprécié des épigrammatistes : la simplicité de la forme permettait aux auteurs de s’illustrer en l’espace de quelques vers. Ainsi, l’épigramme est souvent décrite comme un art de la variation en tant que tel (Taran, 1979). La nature même du corpus anthologique (la réunion de poèmes hétéroclites néanmoins reliés par des topoï communs) en fait une source d’intertextualité inépuisable. Certains thèmes reviennent particulièrement fréquemment, et les épigrammes se répondent, parfois avec plusieurs siècles de décalage (Gutzwiller, 1998). Cette forme (qui s’apparente presque à du plagiat), bien vue et encouragée par les pratiques rhétoriques, commande la production de la littérature grecque et son évolution (Laurens, 2012).\nLa Chaire de recherche du Canada sur les écritures numériques et leurs partenaires travaillent depuis plusieurs années à une édition numérique de l’Anthologie grecque (Vitali-Rosati, 2020 ; Vitali-Rosati, 2021). Cette édition et la structure de données qu’elle a permis de produire constituent le corpus de départ de notre projet. Leur édition numérique de l’Anthologie donne accès au texte original (image et transcription), à diverses traductions multilingues, aux commentaires marginaux du manuscrit (scholies, gloses, (inter)titres, etc.), mais aussi à des commentaires contemporains. La plateforme et le modèle de données sur lequel elle se construit favorisent la mise en évidence des relations intertextuelles présentes à l’intérieur du manuscrit, notamment par l’usage de marqueurs codés (auteurs, thèmes, mots-clés, etc.). À ce titre, et dans la continuité de leurs travaux, la Chaire de recherche a entamé au début de l’année 2022 une expérimentation inédite pour trouver une définition formelle – computationnelle et algorithmique – d’un concept littéraire au moyen des algorithmes de fouille de textes.\n\n\nProblématique\nL’Anthologie grecque propose L’épigramme, forme courte et La forme de l’Anthologie grecque L’Anthologie grecque comme horizon de recherche Le développement d’une problématique autour de l’Anthologie grecque La forme épigrammatique et l’histoire de l’Anthologie grecque offre la possibilité d’étudier Comme corpus, l’Anthologie grecque offre à la fois des contraintes importantes et des défis ; en tant qu’oeuvre littéraire, elle est importante, mais en tant que corpus pour la recherche en intelligence artificielle, sa taille est limitée. Qui plus est, les outils d’études du langage et de modélisation de la langue que nous retrouvons dans certains langages vivants comme le français ou l’anglais n’ont pas été développés pour le grec ancien.\nLa forme épigrammatique par sa longueur, ses sujets et ses contextes (changeant au fil du temps) forme les limites des méthodes utilisables sur un corpus qui en est exclusivement constituté, mais est aussi permissive pour certains algorithmes.\nDonc, une part importante de notre problématique se doit d’être située par les contraintes de la forme et de la portée du corpus (temporelle et textuelle).\nDans sa forme initiale, le projet se veut une formalisation du concept de variation dans l’Anthologie grecque. Notre question de recherche en est une dont les résultats prévus pourront supporter le projet IAL puisque nous nous intéressons à la découverte de variations littéraires parmi les épigrammes de l’Anthologie grecque.\nÉtant conscients des approches déjà mises de l’avant par l’équipe de IAL et des futures étapes proposées par les nouveaux membres du projet, nous avons décider de d’utiliser des outils et méthodes, et de choisir un objectif qui permettrait d’améliorer les chances de réussite de ces autres projets tout en se concentrant sur un autre (set) d’algorithmes d’intelligence artificielle.\nÀ l’aide du One Shot Learning, une méthode de reconnaissance de la similarité entre deux entités vectorielles. Le One Shot Learning offre de très bons résultats dans le domaine de la reconnaissance d’images, un tel algorithme étant capable de juger de la ressemblance entre deux visages ou espèces d’animaux ou de plantes à partir de données très limitées.\nLa découverte de toutes les variations d’un épigramme de l’Anthologie grecque est difficile, entre autres parce que plusieurs formes de variation ont été recensées, mais aussi parce que la taille du corpus dépasse les capacités de travail des humains.\nAinsi, la détection des variations, qui est un enjeu dans le projet initial (puisque de connaître d’avance les variations permettrait d’entraîner un modèle capable de reconnaître les caractéristiques textuelles qui confirment la présence d’une variation), se pose comme tâche heuristique à accomplir."
  },
  {
    "objectID": "posts/carnet/evaluation-logiciel-fouille-de-texte.html",
    "href": "posts/carnet/evaluation-logiciel-fouille-de-texte.html",
    "title": "Évaluation du logiciel Orange Data Mining",
    "section": "",
    "text": "Brouillon de l’évaluation du logiciel Orange Data Mining.\n\n\n\n\n\n\nType de logiciel (libre, propriétaire, etc.)\nType de licence (fixe, libre, académique, mixte, etc.)\nCoût\nLangage de programmation (Python, Java, etc.)\n\n\n\n\n\nÂge du logiciel\nAnnée en affaires de l’entreprise\nNombre de mois depuis la dernière mise à jour\nLe logiciel est-il encore supporté par le développeur ou le créateur\n\n\n\n\nCritères qui impliquent la capacité du logiciel à s’harmoniser avec le système déjà en place et sa faculté de fonctionner conjointement avec d’autres ressources logicielles et matérielles.\n\nCompatibilité avec les plateformes et systèmes d’exploitation\nIntégration Web\nIntégration à d’autres applications\nDépendances logicielles ou matérielles\n\n\n\n\nCritères qui mesurent l’aide disponible pour les utilisateurs du logiciel.\n\nAccès à une version de démonstration\nAssistance technique\nDocumentation\nTutoriel\nCommunauté"
  },
  {
    "objectID": "posts/carnet/evaluation-logiciel-fouille-de-texte.html#évaluation-du-logiciel-orange-data-mining",
    "href": "posts/carnet/evaluation-logiciel-fouille-de-texte.html#évaluation-du-logiciel-orange-data-mining",
    "title": "Évaluation du logiciel Orange Data Mining",
    "section": "Évaluation du logiciel Orange Data Mining",
    "text": "Évaluation du logiciel Orange Data Mining\n\nDescription\n\nType de logiciel : Orange Data Mining (Orange) est un logiciel libre, développé par le laboratoire de bioinformatique (faculté des sciences de l’information et informatique) de l’Université de Llubljana en Slovénie. Nous levons la confusion possible avec la firme française Orange : Orange Data Mining n’a aucun lien avec cette entreprise de services en télécommunication.\nType de licence : Le logiciel est sous GNU General Public License 3.0, les documentations et documents additionnels sont tous sous licence Creative Commons Attribution-ShareAlike.\nCoût : Aucun paiement n’est requis pour utiliser le logiciel, toutefois il est possible d’effectuer une donation pour financer le projet.\nLangage de programmation : La base logicielle est développée en C++, et les widgets, nous y reviendrons plus tard, sont développés avec le langage de programmation Python.\n\n\n\nMaturité du logiciel\n\nÂge du logiciel : La version 3 du logiciel remonte à 2013, avec une première release en 2014 (version 3.1). Orange est développé au tout début de l’essor des algorithmes d’intelligence artificielle et dans le même laps de temps que d’autres librairies (python) telle que ScikitLearn.\nAnnée en affaires de l’entreprise : Le logiciel a été créé et est maintenu par le même laboratoire depuis 2013.\nLe dépôt Git d’Orange est très dense. Les derniers commits remontent tout juste à quelques heures avant l’écriture de ce billet (21 novembre 2022, 18h00). Ce projet comptabilise à son actif pas moins de 14,442 commits, 3800 étoiles, 881 forks, plus de 35 releases (la dernière datant du 11 octobre 2022, release 3.33.0) et pas moins de 92 contributeurs. Un autre élément qui démontre l’activité de ce projet est le nombre d’issues (de problèmes ou questions à résoudre) : nous en observons 91 ouvertes et 2017 fermées.\n\n\n\nInteropérabilité\n\nCompatibilité avec les plateformes et les systèmes d’exploitation : Ce logiciel est compatible avec les systèmes d’exploitation les plus répandus. Nous avons testé de l’installer sous deux OS (Windows 10 et Ubuntu 20.04.5 LTS) et nous n’avons rencontré aucune difficulté. Dans le premier cas, il s’agit de télécharger un fichier exécutable .exe et de l’exécuter, le wizard d’installation s’occupe alors du reste. Du côté d’Ubuntu, l’installation est encore plus simple : elle est réalisable dans le terminal via l’installateur de paquet Python pip ou Anaconda, nous avons choisi la première option : tout s’est déroulé sans le moindre bug.\nIntégration à d’autres applications : Orange n’est pas forcément prévu pour s’inscrire dans un écosystème où les logiciels peuvent communiquer entre eux. Les formats de données utilisables en entrée du processus sont assez larges tant qu’ils respectent la forme d’un tableau (.xlsx, .csv, Google Sheet, table d’une base de données SQL). Par contre, les possibilités d’exports sont plus minces : seules les visualisations générées et les modèles (workflows) peuvent être exportés depuis le logiciel. Cependant, le logiciel offre beaucoup de plasticité, notamment au niveau des méthodes mises en œuvre puisqu’il est possible de développer ses propres widgets en Python et de les utiliser dans Orange, par exemple pour créer des workflows adaptés à des questions de recherche ou des modèles de données peu conventionnels.\nDépendances logicielles ou matérielles : Il n’y aucune dépendance hormis le langage de programmation Python (qui est nativement installé sur les distributions Mac et Linux).\n\n\n\nSoutien technique\nCritères qui mesurent l’aide disponible pour les utilisateurs du logiciel.\n\nAccès à une version de démonstration : Il n’y a pas réellement de version de démonstration. Plusieurs exemples sont fournis à l’intérieur du logiciel et permettent de tester directement différents workflows ou méthodes à appliquer à des corpus de données. D’ailleurs, plusieurs corpus sont également disponibles avec le logiciel. Au-delà de leur emploi et de l’application de calculs sur ces données, il est possible de les ouvrir et regarder la structure des données (fait sans doute important lors d’une phase d’apprentissage des outils d’intelligence artificielle).\nAssistance technique : Il est possible de prendre contact avec la communauté Orange à travers différentes plateformes : le site Web, les réseaux sociaux, ou encore Discord et Stack Exchange.\nDocumentation : Orange fournit des documentations très précises quant aux différents usages du logiciel ou des développements possibles (voir les URLs dans la section références en bas du document). Cependant, nous noterons que la documentation ne décrit pas complètement le comportement des algorithmes que les usagers vont employer.\nTutoriels : Des tutoriels et des exemples sont disponibles un peu partout dans Orange : il est possible de trouver dans le logiciel des workflows génériques prêt à l’emploi, avec des explications pour chacune des étapes à réaliser. L’équipe d’Orange a également publié des tutoriels vidéos sur la plateforme Youtube et des documentations plus poussées pour des utilisateurs avancés. En tant que nouveaux utilisateurs, nous nous sentons vraiment accompagnés dans l’apprentissage et la prise en main de l’outil.\n\n\n\nQuelques essais de l’outil\n\nPrincipe de fonctionnement\nOrange est basé sur le principe de visual programming (comme pour les logiciels Pure Data ou Max/MSP). Une fenêtre s’ouvre et l’utilisateur doit assembler des widgets dans le plan de travail pour créer son workflow : ce sont les différentes étapes et méthodes appliquées au corpus de données. L’utilisateur relie les widgets entre eux pour automatiser la communication des données (in et out) jusqu’à la dernière étape choisie dans le workflow.\nTout workflow est initié par l’importation des données, auxquelles seront appliqués différents traitements en vue d’obtenir les résultats escomptés. Orange est tout autant utilisé pour faire de la fouille de données que de la fouille de textes. D’ailleurs, dans le site Web du logiciel, un système de mot-clefs permet de repérer rapidement les exemples de workflow appartenant à telle ou telle catégorie de traitement.\nCi-dessous un premier test réalisé avec un des jeux de données (test) fourni par le logiciel.\n\nLe jeu de données est un corpus de texte au format .tab.\nLe premier widget que nous appliquerons au corpus sera le preprocess text.\n\nCette phase permet de transformer notre texte (application du lowercase pour enlever les majuscules), de le tokeniser grâce à l’expression régulière \\w+ (pour matcher avec tous les caractères sauf les espaces), puis de le filtrer grâce à une liste de stopwords en anglais.\nLe deuxième widget sera appliqué au corpus preprocessé, il s’agit du widget bag of words.\n\nBag of words permet de générer un compteur pour chaque donnée du document.\nLe troisième widget de ce workflow s’appelle Distances.\n\nIl permet de calculer la distance entre les documents (cosinus). Plus la distance est petite, plus les documents sont proches et similaires.\nLe quatrième widget se nomme Hierarchical Clustering.\n\nCette étape marque le début de la visualisation des résultats après transformation des données : elle permet de visualiser la hiérarchie des documents à travers un dendrogramme.\nLe dernier widget, encore une visualisation, est intitulé MDS.\n\nIl offre une visualisation spatiale de nos données réalisée grâce à un autre calcul de distance.\n\n\n\nCommentaires\nUn autre test a été réalisé avec notre corpus d’épigrammes au format .csv (français, anglais, grec ancien) et il n’a pas été aussi simple d’appliquer ce workflow à nos données. Les résultats obtenus ne faisaient pas sens : autant la génération d’un nuage de mot (avec le widget wordcloud) est très facile, autant les calculs de distance et la création de clusters hiérarchisés ne fonctionnait pas. Le problème est relativement simple à cibler : l’ordinateur utilisé pour le test surchauffe et n’est pas assez puissant pour traiter ce corpus (le logiciel “ne répond plus”).\nOrange est un logiciel confortable à utiliser, la prise en main se fait très rapidement et la documentation (très fournie) accompagne l’utilisateur tout au long de son apprentissage. On trouve des informations sur le site Web du logiciel ainsi qu’une multitude de petits tutoriels sur YouTube pour expliquer chacune des étapes. Cependant, dès les premières manipulations, nous avons ressenti une légère frustration (relative au visual programming) car les widgets, certes très intuitifs, camouflent le fonctionnement des algorithmes appliqués au corpus. L’agencement des widgets entre eux permet de bien voir le parcours de nos données, mais cela rend les calculs appliqués opaques. N’étant pas expert dans le domaine de la fouille de texte (plutôt néophyte) il est plus difficile de comprendre comment fonctionne exactement chaque widget. Par exemple, le widget Distances ne propose qu’un petit sélecteur pour le type de distance à calculer, mais que se passe-t-il sous ce sélecteur ? Nous obtenons des résultats, mais sans comprendre comment le logiciel les a générés."
  },
  {
    "objectID": "posts/carnet/evaluation-logiciel-fouille-de-texte.html#références",
    "href": "posts/carnet/evaluation-logiciel-fouille-de-texte.html#références",
    "title": "Évaluation du logiciel Orange Data Mining",
    "section": "Références",
    "text": "Références\n\nDemsar J, Curk T, Erjavec A, Gorup C, Hocevar T, Milutinovic M, Mozina M, Polajnar M, Toplak M, Staric A, Stajdohar M, Umek L, Zagar L, Zbontar J, Zitnik M, Zupan B (2013) Orange: Data Mining Toolbox in Python, Journal of Machine Learning Research 14(Aug): 2349−2353.\nSite Web du logiciel : https://orangedatamining.com/\nDocumentation Data Mining Library : https://orange3.readthedocs.io/projects/orange-data-mining-library/en/latest/index.html#\nDocumentation utilisateurs (visual programming) : https://orange3.readthedocs.io/projects/orange-visual-programming/en/latest/index.html\nDocumentation développements : https://orange3.readthedocs.io/projects/orange-development/en/latest/index.html\nGitHub du projet : https://github.com/biolab/orange3"
  },
  {
    "objectID": "posts/carnet/Mikolov.html",
    "href": "posts/carnet/Mikolov.html",
    "title": "Revue de littérature : Mikolov et al.",
    "section": "",
    "text": "Court résumé de l’article de Mikolov, Tomas et al."
  },
  {
    "objectID": "posts/carnet/Mikolov.html#efficient-estimation-of-word-representations-in-vector-space",
    "href": "posts/carnet/Mikolov.html#efficient-estimation-of-word-representations-in-vector-space",
    "title": "Revue de littérature : Mikolov et al.",
    "section": "Efficient Estimation of Word Representations in Vector Space",
    "text": "Efficient Estimation of Word Representations in Vector Space\nLa librairie Word2Vec et la vectorisation de mots sont le résultat d’une démarche téléologique précisée à même le titre de cet article : l’approximation d’une représentation vectorielle des mots (ou formes) qui soit efficace. Dans un contexte de recherche d’une intelligence artificielle générale (voir GPT-3 et LaMDA), cette intention, mise de l’avant par Mikolov et ses collègues doit servir rappel des contraintes épistémologiques de leur démarche : la vectorisation de mots ne génère pas un objet mathématique qui représente la sémantique de ce dernier. Plutôt, la vectorisation de mot donne une approximation dans un espace vectoriel de l’utilisation normale d’une forme en fonction de plusieurs paramètres, le premier étant nommé le contexte. Ce paramètre décrit combien de formes seront considérés à la fois, car Word2Vec considère chaque formes en fonction de celles qui l’entourent dans le corpus ; le contexte est une valeur entière qui exprime combien de formes sont considérés avant et après chaque forme. L’algorithme de Word2Vec minimise une fonction de perte selon l’une des deux architectures choisies ; la première, Continuous Bag of Word (CBOW) utilise un réseau de neurones pour prédire quel terme est le plus probable à trouver en fonction d’un contexte donné, puis vérifie sa prédiction en fonction du texte. À l’inverse, le Skip-gram prédit quels formes feront partie du contexte à partir d’un terme, puis corrige en fonction de la réponse correcte tirée du corpus. Cet algorithme entraîne son réseau de neurones sur chaque forme (et contexte) du corpus un nombre de fois (epoch) choisi par l’utilisateur\nLa vectorisation de mot est une méthode rendue possible par la disponibilité de corpus massifs (dont le nombre de formes se compte en milliards) et fonctionne à partir de plusieurs présuppositions :\n\nLa linguistique distributionnelle : il est possible d’inférer le sens des mots à partir de leur position et distribution dans des textes;\nLa régularité sémantique : les mots ayant des sens semblables auront des vecteurs ayant plusieurs degrés de similarité;\nLa similarité syntaxique : les mots étant utilisés de manière similaire auront des vecteurs ayant plusieurs degrés de similarité.\n\nUne importante découverte de ce modèle est que, pour un corpus assez grand, ces vecteurs ont des propriétés additives intéressantes. L’exemple cité par les auteurs est le suivant : \\(\\overrightarrow{King} - \\overrightarrow{Man} + \\overrightarrow{Woman} \\approx \\overrightarrow{Queen}\\). En d’autres mots, il existe une certaine cohérence dans l’espace vectoriel produit, ce dernier encode les similarités syntaxiques et sémantiques sous la forme de distances plus courtes entre deux formes similaires, mais aussi les translations sémantiques. Ainsi, une paire de mots dont la différence vectorielle est la même que pour une autre paire de mots dénote une relation sémantique similaire. C’est d’ailleurs l’une des deux manières par laquelle les auteurs évaluent leurs modèles : testant sa capacité à produire des listes de paires de mots sémantiquement liés, par exemple des pays et leurs capitales ou devises. Leur autre outil d’évaluation des modèles est syntaxique, ils y testent des paires comme celles des adjectifs et leurs adverbes, ou un mot et son contraire. Les modèles proposés par les auteurs dépassent 50% de précision à ces tests, les auteurs précisent que de plus gros modèles (avec des vecteurs et corpus plus imposants) seront sans doute en mesure de présenter de meilleurs résultats."
  },
  {
    "objectID": "posts/carnet/buchler_unsupervised.html",
    "href": "posts/carnet/buchler_unsupervised.html",
    "title": "Revue de littérature : Buchler et al.",
    "section": "",
    "text": "Court résumé de l’article de Büchler, Marco; Geßner, Annette; Eckart, Thomas; Heyer, Gerhard. (2010)."
  },
  {
    "objectID": "posts/carnet/buchler_unsupervised.html#büchler-marco-geßner-annette-eckart-thomas-heyer-gerhard.-2010.-unsupervised-detection-and-visualisation-of-textual-reuse-on-ancient-greek-texts.-colloquium-on-digital-humanities-and-computer-science.",
    "href": "posts/carnet/buchler_unsupervised.html#büchler-marco-geßner-annette-eckart-thomas-heyer-gerhard.-2010.-unsupervised-detection-and-visualisation-of-textual-reuse-on-ancient-greek-texts.-colloquium-on-digital-humanities-and-computer-science.",
    "title": "Revue de littérature : Buchler et al.",
    "section": "Büchler, Marco; Geßner, Annette; Eckart, Thomas; Heyer, Gerhard. (2010). Unsupervised Detection and Visualisation of Textual Reuse on Ancient Greek Texts. Colloquium on Digital Humanities and Computer Science.",
    "text": "Büchler, Marco; Geßner, Annette; Eckart, Thomas; Heyer, Gerhard. (2010). Unsupervised Detection and Visualisation of Textual Reuse on Ancient Greek Texts. Colloquium on Digital Humanities and Computer Science.\nCet article se concentre sur l’étude des références indiquées dans les textes en grec ancien : grâce aux références mentionnées par les auteurs, il est possible de vérifier ou questionner le texte d’une édition d’un texte en grec ancien. Dans cet article, les liens entre ces passages de texte sont appelés Reuse Graph G. Avec ce type de graphe il est possible de quantifier le degré d’importance de la réutilisation d’un texte (traduction littérale de l’article, les auteur.e.s préfèrent l’emploi de reuse text plutôt que citation) d’un auteur par rapport aux autres auteurs à travers le temps.\nMalgré la petite taille du corpus en grec ancien, en comparaison des corpus disponible dans différentes langues vivantesm il est quasiment impossible de référencer et d’étiqueter tous les passages de textes réutilisés manuellement : ce qui démontre l’intérêt d’utiliser un ordinateur pour ce travail. Un des corpus en grec ancien les plus utilisés est le TLG (Thesaurus Linguae Graecae), largement annoté depuis plusieurs décennies.\nLes méthodes de comparaisons des paires de liens entre deux passages sont extrêmement longues, même à raison de 1000 comparaisons par seconde, elles sont donc écartées pour cette étude.\nLa première étape est celle de la segmentation du corpus : différents marqueurs dans le texte, combinés avec des listes d’abréviations, permettent d’améliorer cette segmentation et la détection des fragments de textes réutilisés. Concernant la tokenisation, en addition des marqueurs de ponctuation, tous les crochets de la convention Leiden sont supprimés. Les résultats montrent une segmentation du TLG en 5 520 060 phrases selon une moyenne de 13.51 mots par phrase. L’étape de normalisation passe par la mise en minuscule du corpus, car plusieurs mots peuvent être écrits selon des variantes en minuscule ou majuscule, rendant la tâche plus délicate pour les algorithmes sensibles à la casse qui de fait ignore un passage de texte réutilisé. Une autre variation peut générer des erreurs : la variation morphologique. Pour parer ce problème, les chercheurs ont recours au processus de Lemmatisation en utilisant l’analyseur morphologique Morpheus pour réduire les mots à leur forme la plus simple. Ensuite, deux approches syntaxiques sont testées et comparées : une approche basée sur la statistique de l’expansion des n-grams et une autre approche, sémantique, basée sur la segmentation des unités linguistiques. La première approche permet de trouver la plus longue correspondance commune d’une réutilisation avec le texte original. Une conséquence négative de cette approche est que tous les préfixes communs de la plus longue correspondances (au moins 5 mots) sont produits. En conséquence de quoi il faut prévoir une étape supplémentaire de post-procesing pour supprimer les préfixes. Toutefois, la réutilisation de texte n’est pas seulement en rapport avec les mots mais également avec le sens des idées transmises : une approche sémantique est nécessaire. La méthode employée est celle des sub-graphs, et consiste en l’élaboration de graphes de co-occurence sémantique (association de réseau de mots d’un corpus décrit par un ensemble de mots uniques. et un ensemble d’associations). Pour le corpus du TLG, plus de 300 millions de co-occurences ont été calculées (en comptabilisant les 10% de stopwords). Dernière étape de la méthode : la visualisation. Les auteurs distinguent deux niveaux de visualisation : macro et micro. La vue macro se rapportent à des diagrammes traditionnels alors que la vue micro est visualisée par des surlignements du texte pour mettre en valeur les variations dans les correspondances.\nLes résultats obtenus avec cette méthode montre que la similarité des correspondances entre les fragments de texte n’est quasiment jamais identique. Elle varie entre un score allant de 0.2 jusqu’à 0.9, et le score le plus élevé sur toutes les références ne dépasse pas les 18% (score de 0.8)."
  },
  {
    "objectID": "posts/carnet/Lampinen_2018.html",
    "href": "posts/carnet/Lampinen_2018.html",
    "title": "Revue de littérature : Lampinen et McCelland",
    "section": "",
    "text": "Court résumé de l’article de Lampinen, Andrew K. et James L. McCelland."
  },
  {
    "objectID": "posts/carnet/Lampinen_2018.html#one-shot-and-few-shot-learning-of-word-embeddings",
    "href": "posts/carnet/Lampinen_2018.html#one-shot-and-few-shot-learning-of-word-embeddings",
    "title": "Revue de littérature : Lampinen et McCelland",
    "section": "One-shot and Few-shot Learning of Word Embeddings",
    "text": "One-shot and Few-shot Learning of Word Embeddings\nLes méthodes d’apprentissage machine demandent des milliers ou millions d’exemples avant d’intégrer un concept ; pire encore, il est très difficile de leur apprendre de nouveaux concepts une fois qu’ils sont entraînés. Dans cet article, Lampinen et McClelland proposent de créer une intelligence artificielle capable de s’adapter à de nouvelles formes et contexte à partir des algorithmes de one-shot learning et few-shot learning. Cette approche pourrait servir d’alternative aux méthodes modernes entraînées sur des milliards de mots en permettant de créer un modèle capable de se perfectionner à chaque utilisation. Pour l’instant, une méthode permet d’ajouter des cas à un modèle déjà entraîné : la rétropropagation du gradient qui permet de conserver le savoir déjà acquis en gelant les poids qui ne sont pas directement liés au nouveau terme. Les auteurs s’inspirent donc de cette technique et s’attaquent à la tâche de la prédiction automatique du mot suivant d’une phrase. Leurs résultats sont probants : la méthode étant souvent plus efficace qu’une vectorisation de mots classique (dans trois des quatre cas de figure considérés), quoique les modèle ait été affecté par les nouvelles formes ajoutées. Plus précisément, la perplexité (la mesure d’à quel point un modèle prédit un échantillon avec succès) est amoindrie pour des mots étalons choisis par les auteurs ; une étude plus exhaustive du modèle (100 formes) montre que la perplexité moyenne est similaire à celle obtenue avec un entraînement sur le corpus entier. La méthode proposée par les auteurs offre une meilleure distinction entre les différents contextes en comparant les probabilités qu’une forme apparaisse dans le contexte donné avec d’autres méthodes. Selon les auteurs, le one-shot learning a toutefois une faiblesse. Inspirés des travaux de Kumaran sur la complementary learning systems theory, ils indiquent que leur succès avec des schema-constistent knowlege (comme des données textuelles unilingues) ne sera pas réplicable avec des schema-inconsistent knowledge (des données qui sont différentes des données d’entraînement, par exemple l’intrusion d’une autre langue)."
  },
  {
    "objectID": "posts/carnet/craneEtBamman.html",
    "href": "posts/carnet/craneEtBamman.html",
    "title": "Revue de littérature : Crane et Bamman",
    "section": "",
    "text": "Court résumé de l’article de Bamman, D et Crane, G. (2009)."
  },
  {
    "objectID": "posts/carnet/craneEtBamman.html#bamman-d.-et-crane-g.-2009.-structured-knowledge-for-low-resource-languages-the-latin-and-ancient-greek-dependency-treebanks.",
    "href": "posts/carnet/craneEtBamman.html#bamman-d.-et-crane-g.-2009.-structured-knowledge-for-low-resource-languages-the-latin-and-ancient-greek-dependency-treebanks.",
    "title": "Revue de littérature : Crane et Bamman",
    "section": "Bamman, D. et Crane, G. (2009). Structured Knowledge for Low-Resource Languages: The Latin and Ancient Greek Dependency Treebanks.",
    "text": "Bamman, D. et Crane, G. (2009). Structured Knowledge for Low-Resource Languages: The Latin and Ancient Greek Dependency Treebanks.\nCet article sert à décrire la création de corpus arborés (de larges collections de données annotées) pour le latin et le grec ancien. Deux objectifs sont poursuivis pour ces corpus classiques : découvrir des connaissances lexicales et identifier des modèles de réutilisation de textes.\nContexte : les techniques de fouille de texte expriment leur potentiel pour des corpus généralement anglais ou germanophone car ces derniers sont très étendus. Les ressources disponibles pour les corpus en langues anciennes (comme le grec et le latin) parraissent bien pâles à côté. Le constat réalisé tend à montrer que le bon fonctionnement des algorithmes de fouille de texte est quantitativement proportionnel à la quantité de textes disponibles dans un corpus : verdict, Bamman et Crane estiment que toutes les informations d’un texte ne sont pas extraites et qu’ils peuvent s’y atteler, notamment sur les corpus sous-représentés comme latin et grec.\nLe corpus sur lesquels s’appuie cette étude sont composés d’une large collection de phrases syntaxiquement annotées, appelée treebanks, dont chaque mot fait l’objet d’une explicitation de sa relation aux autres mots de la phrase dans laquelle il s’inscrit. Ces données peuvent être représentées sous forme de graphes puis encodées en XML. Les corpus latins utilisés sont ceux de la Prague Dependency Treebank, de l’Index Thomisticus et le corpus PROIEL du Nouveau Testament. Les corpus en grec ancien sont plus larges et comportent des écrits tels que les tragédies grecques, Platon, les récits d’Homère (L’Iliade et L’Odyssée), et bien d’autres.\nLa première approche des corpus est lexicale : elle permet de définir ce que chaque mot veut dire et ses interactions avec les autres mots. Ensuite, la deuxième méthode appliquée au corpus consistait à mettre les corpus et leurs traductions dans différentes langues en parallèle pour trouver le sens des mots par équivalence. Ces informations permettent de mieux définir les relations entre les mots. Les résultats obtenus dans la plupart des recherches sur la réutilisation de texte (et leurs variations) fonctionne comme indiqué précédemment : les méthodes sont adatpées aux langues vivantes mais pas réellement au corpus des langues anciennes. Malgré la petite taille du corpus (50 000 mots), les méthodes permettent d’élaborer 12 caractéritisques syntaxiques pour chaque mot dans une phrase (des combinaisons de la représentation au niveau du mot, sous la forme de token, de lemme ou simplement de partie du discours)."
  }
]